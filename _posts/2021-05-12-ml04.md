---
layout : single
title: "머신러닝 : 로지스틱 회귀 구현"
author : CMK
use_math: true
---



## 과제 1 : 배치 경사 하강법으로 로지스틱 회귀 구현하기

조기 종료를 사용한 배치 경사하강법으로 로지스틱 회귀를 구현하고자 한다.

단, 사이킷런은 전혀 사용하지 않는다.



구현 과정은 크게 다음과 같다.

1. 데이터 준비
2. 데이터셋 분할
3. 로지스틱 회귀 함수 구현
4. 경사하강법 훈련
5. 규제가 추가된 경사하강법 훈련
6. 조기 종료 추가
7. 테스트 세트평가



**단계 1 : 데이터 준비**

우선 공동모듈을 임포트 한다.

```python
import numpy as np
```



사이킷런에서 제공하는 붓꽃(iris) 데이터셋을 불러온다.

```python
from sklearn import datasets
iris = datasets.load_iris()
```



꽃잎 길이와 꽃잎 너비 특성을 이용하여 버지니카 품종 여부를 판정하는 데에 사용되는 데이터셋을 지정한다.

```python
X = iris["data"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 넓이 특성 사용
y = (iris["target"] == 2).astype(np.int) # Virginica 품종일 때 1 (양성)
```



모든 샘플에 편향을 추가한다. 이유는 아래 수식을 행렬 연산으로 보다 간단하게 처리하기 위해 0번 특성값 $x_0$이 항상 1이라고 가정하기 때문이다.


$$
\theta_0\cdot 1 + \theta_1\cdot x_1 + \cdots + \theta_n\cdot x_n =

\theta_0\cdot x_0 + \theta_1\cdot x_1 + \cdots + \theta_n\cdot x_n
$$


```python
X_with_bias = np.c_[np.ones([len(X), 1]), X]
```

결과를 일정하게 유지하기 위해 랜덤 시드를 지정한다.

```python
np.random.seed(2042)
```

 

**단계 2: 데이터셋 분할**

데이터셋을 훈련, 검증, 테스트 용도로 6대 2대 2의 비율로 무작위로 분할한다.

* 훈련 세트 : 60%
* 검증 세트 : 20%
* 테스트 세트 : 20%

사이킷런의 `train_test_split()` 함수를 사용하지 않고 수동으로 무작위 분할한다. 먼저 각 세트의 크기를 결정한다.

```python
test_ratio = 0.2                                         # 테스트 세트 비율 = 20%
validation_ratio = 0.2                                   # 검증 세트 비율 = 20%
total_size = len(X_with_bias)                            # 전체 데이터셋 크기

test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%
validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%
train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%
```



`np.random.permutation()` 함수를 이용하여 인덱스를 무작위로 섞는다.

```python
rnd_indices = np.random.permutation(total_size)
```



무작위로 섞인 인덱스를 이용하여 지정된 6:2:2의 비율로 훈련, 검증, 테스트 세트로 분할한다.

```python
X_train = X_with_bias[rnd_indices[:train_size]]
y_train = y[rnd_indices[:train_size]]

X_valid = X_with_bias[rnd_indices[train_size:-test_size]]
y_valid = y[rnd_indices[train_size:-test_size]]

X_test = X_with_bias[rnd_indices[-test_size:]]
y_test = y[rnd_indices[-test_size:]]
```



**단계 3 : 로지스틱 회귀 함수 구현**

다음과 같은 시그모이드 함수를 코드로 구현한다.


$$
\sigma(t) = \frac{1}{1 + e^{-t}}
$$


```python
def sigmoid(x):
  return 1/(1 + np.exp(-x))
```



**단계 4 : 경사하강법 훈련**

특성별 파라미터로 이루어진 (3,1) 크기의 2차원 넘파이 어레이 Θ를 무작위로 초기 설정한다.

```python
Theta = np.random.randn(3, 1)
```



배치 경사하강법을 구현한다.

```python
eta = 0.01
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
y_train = y_train.reshape(90,1)

for iteration in range(n_iterations):     # 5001번 반복 훈련
    z = X_train.dot(Theta)
    y = sigmoid(z)
    
    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력
        loss = -np.mean(np.sum(y_train*np.log(y+epsilon) + (1- y_train)*np.log(1-y+epsilon), axis=1))
        print(iteration, loss)
    
    error = y - y_train                   #그레디언트 계산
    gradients = 1/m*X_train.T.dot(error)   

    Theta = Theta - eta * gradients     # 파라미터 업데이트
```



검증 세트에 대한 예측과 정확도를 확인한다.

```python
z = X_valid.dot(Theta)             
y = sigmoid(z)
y_predict = np.where(y > 0.5, 1, 0)
y_valid = y_valid.reshape(30,1)

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```



**단계 5 : 규제가 추가된 경사하강법 훈련**

$l_2$ 규제가 추가된 경사하강법 훈련을 구현한다.

손실(비용)에 $ℓ_2$ 페널티가 추가되었고 그래디언트에도 항이 추가되었다.

(`Theta`의 첫 번째 원소는 편향이므로 규제하지 않는다.)

```python
eta = 0.1            			#학습률 증가됨
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
y_train = y_train.reshape(90,1)
alpha = 0.1                     #규제 하이퍼파라미터

Theta = np.random.randn(3, 1)   # 파라미터 새로 초기화

for iteration in range(n_iterations):     # 5001번 반복 훈련
    z = X_train.dot(Theta)
    y = sigmoid(z)
    
    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력
        xentropy_loss = -np.mean(np.sum(y_train*np.log(y+epsilon) + (1-y_train)*np.log(1-y+epsilon), axis=1))
        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))    #편향은 규제에서 제외
        loss =  xentropy_loss + alpha * l2_loss         # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = y - y_train                   				#그레디언트 계산
    l2_loss_gradients = np.r_[np.zeros([1, 1]), alpha * Theta[1:]]   # l2 규제 그레이디언트
    gradients = 1/m*X_train.T.dot(error) + l2_loss_gradients

    Theta = Theta - eta * gradients     				# 파라미터 업데이트
```



```python
z = X_valid.dot(Theta)             
y = sigmoid(z)
y_predict = np.where(y > 0.5, 1, 0)
y_valid = y_valid.reshape(30,1)

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```

검증 세트에 대한 정확도를 확인해보면, 추가된 $l_2$ 패널티 때문에 손실이 조금 커지고, 정확도도 떨어지는 것을 확인할 수 있다.



**단계 6 : 조기 종료 추가**

위 규제가 사용된 모델의 훈련 과정에서 매 에포크마다 검증 세트에 대한 손실을 계산하여, 오차가 줄어들다가 증가하기 시작할 때 멈추도록 한다.

```python
eta = 0.1            
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
y_train = y_train.reshape(90,1)
y_valid = y_valid.reshape(30,1)
alpha = 0.1               #규제 하이퍼파라미터
best_loss = np.infty      # 최소 손실값 기억 변수

Theta = np.random.randn(3, 1)   #파라미터 새로 초기화

for iteration in range(n_iterations):
  # 훈련 및 손실 계산
  z = X_train.dot(Theta)
  y = sigmoid(z)
  error = y - y_train
  l2_loss_gradients = np.r_[np.zeros([1, 1]), alpha * Theta[1:]]
  gradients = 1/m*X_train.T.dot(error) + l2_loss_gradients
  Theta = Theta - eta * gradients

  # 검증 세트에 대한 손실 계산
  z = X_valid.dot(Theta)
  y = sigmoid(z)
  xentropy_loss = -np.mean(np.sum(y_valid*np.log(y+epsilon) + (1- y_valid)*np.log(1-y+epsilon), axis=1))
  l2_loss = 1/2 * np.sum(np.square(Theta[1:]))
  loss =  xentropy_loss + alpha * l2_loss

  # 500 에포크마다 검증 세트에 대한 손실 출력
  if iteration % 500 == 0:
    print(iteration, loss)
  
  # 에포크마다 최소 손실값 업데이트
  if loss < best_loss:
    best_loss = loss
  else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
    print(iteration - 1, best_loss)          # 종료되지 이전 에포크의 손실값 출력
    print(iteration, loss, "조기 종료!")
    break 
```

```
z = X_valid.dot(Theta)             
y = sigmoid(z)
y_predict = np.where(y > 0.5, 1, 0)
y_valid = y_valid.reshape(30,1)

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```

조기 종료가 추가된 모델의 검증 세트에 대한 정확도는 변함이 없었다.



**단계 7 : 테스트 세트 평가**

마지막으로 테스트 세트에 대한 모델의 최종 성능을 정확도로 측정한다. 모델의 성능이 조금 높아졌다.

```python
z = X_test.dot(Theta)             
y = sigmoid(z)
y_predict = np.where(y > 0.5, 1, 0)
y_test = y_test.reshape(30,1)

accuracy_score = np.mean(y_predict == y_test)  # 정확도 계산
accuracy_score
```



